{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f742e103",
   "metadata": {},
   "source": [
    "# Cat detection model\n",
    "pruned YOLOv8 using COCO dataset to create a lightweight cat detection model\n",
    "\n",
    "## To do\n",
    "Head pruning\n",
    "- ❌ Remove segmentation/pose estimation - not needed for default model\n",
    "- ❌ Reduce box regression granularity - poor idea for small object detection\n",
    "- ✅ Remove unused classes\n",
    "- ✅ Update dataset to reflect pruning\n",
    "- ❌ Remove P5 + retrain - made performance worse\n",
    "- ❓ Add P2 + retrain\n",
    "\n",
    "Whole model\n",
    "- Prune\n",
    "- ✅ Quantise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ae21a",
   "metadata": {},
   "source": [
    "## Configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), os.pardir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b033b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ff3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ea4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_best_device()\n",
    "eval = \"4000\"\n",
    "max_dets = 5\n",
    "imgsz = 960"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f379aa",
   "metadata": {},
   "source": [
    "## Load and evaluate original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original YOLO model\n",
    "model_original = YOLO(os.path.join(\"..\", \"models\", \"yolov8n.pt\"))\n",
    "_ = model_original.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4805c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate original model on dataset with no large objects\n",
    "metrics_original = model_original.val(\n",
    "    data=os.path.join(\"..\", f\"coco{eval}_nolarge.yaml\"),\n",
    "    classes=[0, 15],\n",
    "    max_det=max_dets,\n",
    "    imgsz=imgsz,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a85a5d1",
   "metadata": {},
   "source": [
    "## Structural pruning of detection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original model\n",
    "model_pruned = copy.deepcopy(model_original)\n",
    "_ = model_pruned.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural pruning of unused classes\n",
    "\n",
    "# define new classes\n",
    "keep_classes = [0, 15]\n",
    "new_nc = len(keep_classes)\n",
    "\n",
    "# extract detection head\n",
    "detect = model_pruned.model.model[-1]\n",
    "\n",
    "# loop over each of the multi-scale heads\n",
    "for i, conv in enumerate(detect.cv3):\n",
    "\n",
    "    # copy last layer - same architecture but reduced number of classes\n",
    "    old_conv = conv[-1]\n",
    "    new_conv = torch.nn.Conv2d(\n",
    "        in_channels=old_conv.in_channels,\n",
    "        out_channels=new_nc,\n",
    "        kernel_size=old_conv.kernel_size,\n",
    "        stride=old_conv.stride,\n",
    "        padding=old_conv.padding,\n",
    "        bias=True,\n",
    "    )\n",
    "\n",
    "    # copy weights to new layer\n",
    "    with torch.no_grad():\n",
    "        new_conv.weight.copy_(old_conv.weight[keep_classes])\n",
    "        new_conv.bias.copy_(old_conv.bias[keep_classes])\n",
    "    new_conv.requires_grad_(False)\n",
    "\n",
    "    # insert layer back into head\n",
    "    conv[-1] = new_conv\n",
    "\n",
    "# update metadata\n",
    "model_pruned.model.names = {0: \"person\", 1: \"cat\"}\n",
    "detect.no = detect.no - detect.nc + new_nc\n",
    "detect.nc = new_nc\n",
    "\n",
    "# save model to disk (also syncs some important thing seemingly)\n",
    "model_pruned.save(os.path.join(\"..\", \"models\", \"yolov8n_pruned.pt\"))\n",
    "model_pruned = YOLO(os.path.join(\"..\", \"models\", \"yolov8n_pruned.pt\"))\n",
    "_ = model_pruned.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e27814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pruned model\n",
    "metrics_pruned = model_pruned.val(\n",
    "    data=os.path.join(\"..\", f\"coco{eval}_nolarge_subclass_downsample.yaml\"),\n",
    "    classes=[0, 1],\n",
    "    max_det=max_dets,\n",
    "    imgsz=imgsz,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Structural pruning of unnecessary large object detection\n",
    "\n",
    "# # extract detection head\n",
    "# detect = model_pruned.model.model[-1]\n",
    "\n",
    "# # remove last layer - corresponding to large object detection\n",
    "# detect.nl = 2\n",
    "# detect.cv2 = detect.cv2[: detect.nl]\n",
    "# detect.cv3 = detect.cv3[: detect.nl]\n",
    "# detect.f = detect.f[: detect.nl]\n",
    "# detect.stride = torch.tensor([8, 16, 32][: detect.nl], dtype=torch.float32)\n",
    "\n",
    "# # save model to disk (also syncs some important thing seemingly)\n",
    "# model_pruned.save(os.path.join(\"..\", \"models\", \"yolov8n_pruned.pt\"))\n",
    "# model_pruned = YOLO(os.path.join(\"..\", \"models\", \"yolov8n_pruned.pt\"))\n",
    "# _ = model_pruned.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate pruned model\n",
    "# metrics_pruned = model_pruned.val(\n",
    "#     data=os.path.join(\"..\", f\"coco{eval}_nolarge_subclass_downsample.yaml\"),\n",
    "#     classes=[0, 1],\n",
    "#     max_det=max_dets,\n",
    "#     imgsz=imgsz,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b80556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrain the model head\n",
    "\n",
    "# # reload model to avoid inference mode lockin\n",
    "# model_pruned = YOLO(os.path.join(\"..\", \"models\", \"yolov8n_pruned.pt\"))\n",
    "# _ = model_pruned.model.to(device)\n",
    "\n",
    "# # unfreeze head only\n",
    "# for p in model_pruned.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "# for p in model_pruned.model.model[-1].parameters():\n",
    "#     p.requires_grad = True\n",
    "\n",
    "# # retrain the model\n",
    "# model_pruned.train(\n",
    "#     data=os.path.join(\"..\", \"coco_nolarge_subclass_downsample.yaml\"),\n",
    "#     epochs=30,\n",
    "#     imgsz=imgsz,\n",
    "#     lr0=1e-4,\n",
    "#     patience=3,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47611a61",
   "metadata": {},
   "source": [
    "## Quantise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantise model\n",
    "model_pruned.export(format=\"onnx\", imgsz=imgsz, opset=13, simplify=True)\n",
    "quantize_dynamic(\n",
    "    \"../models/yolov8n_pruned.onnx\",\n",
    "    \"../models/yolov8n_quantised.onnx\",\n",
    "    weight_type=QuantType.QUInt8,\n",
    ")\n",
    "model_quant = YOLO(os.path.join(\"..\", \"models\", \"yolov8n_pruned_q.onnx\"), task=\"detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate quantised model\n",
    "metrics_pruned = model_quant.val(\n",
    "    data=os.path.join(\"..\", f\"coco{eval}_nolarge_subclass_downsample.yaml\"),\n",
    "    classes=[0, 1],\n",
    "    max_det=max_dets,\n",
    "    imgsz=imgsz,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879b5aa",
   "metadata": {},
   "source": [
    "## Plot predictions made on sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbf6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_all = plt.subplots(ncols=3, nrows=2, figsize=(15, 6))\n",
    "sample_images_dir = os.path.join(\"..\", \"sample_images\")\n",
    "for ax, img_pth in zip(ax_all.flatten(), os.listdir(sample_images_dir)):\n",
    "    res = model_quant.predict(\n",
    "        os.path.join(sample_images_dir, img_pth), max_det=max_dets, imgsz=imgsz\n",
    "    )[0]\n",
    "    ax.imshow(res.plot()[:, :, ::-1])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpi-cat-behaviour-monitoring (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
